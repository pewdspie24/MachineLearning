{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural_Network_with_Propagation(Linear-Equation_Method).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"geMqcIugSO_I"},"source":["from random import random\n","from random import seed\n","from math import exp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMwQnEDBRAyD"},"source":["\n","#******************************** Intialize NN ************************\n","\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","  NN = list()\n","  hidden_layer = list()# [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","  for i in range(n_hidden):\n","    hidden = list()\n","    for j in range(n_inputs+1):\n","      hidden.append(random())\n","    hidden_layer.append(hidden)\n","  output_layer = list() #[{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","  for i in range(n_outputs):\n","    output = list()\n","    for j in range(n_hidden+1):\n","      output.append(random())\n","    output_layer.append(output)\n","  NN.append(hidden_layer)\n","  NN.append(output_layer)\n","  return NN\n"," \n","#******************************** Forward Propagation **************************\n","#Forward Propagation: truyền 1 tín hiệu đầu vào, cho xuyên suốt toàn bộ các layer cho tới khi ra được output\n","#Sử dụng nhằm predict new data\n","#Chúng ta chia thành 3 phần\n"," \n","#1. Neuron activation (hàm kích hoạt neuron)- (nhưng dành cho 1 neuron, chỉ là tổng tích weights và input của neuron đó)\n","def activate(weights, inputs):\n","  activation = weights[-1]*1 #bias, mặc định có input = 1 và có thứ tự ở cuối weights\n","  for i in range(len(weights)-1):\n","    activation += weights[i]*inputs[i] #cộng tất cả các tham số đầu vào (weight_i*input_i)\n","  return activation #return về 1 số\n"," \n","#2. Neuron transfer (hay còn gọi là activation function) đây chính là lúc đưa output đã activate thành output chuẩn dạng (Eg: sgn, sigmoid...)\n"," \n","def transfer(activation):\n","  return 1.0/(1.0+exp(-activation)) #Sigmoid\n"," \n","#3. Forward Propagation\n","#forward propagation là quá trình các tính toán của mỗi layer, bao gồm việc activate các neuron, transfer các kết quả vừa ra và đưa row output vừa xong thành input của layer tiếp theo\n"," \n","def forward_propagate(network, row):\n","  inputs = row\n","  for layer in network:\n","    new_inputs = list()\n","    # print(layer,\"kkk\")\n","    for neurons in layer:\n","      #print(neurons)\n","      activated = activate(neurons, inputs)\n","      new_neuron = transfer(activated)\n","      new_inputs.append(new_neuron)\n","    inputs = new_inputs\n","  return inputs\n"," \n","#thử demo 2 input value(2 neuron), 1 hidden layer(1 neuron) và 1 output(2 neurons)\n","#2 input là 0 vs 1, hidden có 2 neuron và 1 bias\n","#5 dòng dưới đây là test forward\n","\"\"\"network = [[[0.13436424411240122, 0.8474337369372327, 0.763774618976614]],\n","        [[0.2550690257394217, 0.49543508709194095],[0.4494910647887381, 0.651592972722763]]]\n","row = [1, 0, None]\n","output = forward_propagate(network, row)\n","print(output)\"\"\"\n","#*********************************Backpropagation**********************************\n","#Backprogation dùng nhằm train các weights trong NN\n","#Chúng ta chia làm 2 phần:\n","#1. Transfer Derivative (hay còn gọi là đạo hàm của activation function)\n"," \n","def transfer_derivative(output):\n","  return output*(1.0-output) #đạo hàm của sigmoid là x*(1-x)\n"," \n","#2. Error Backpropagation\n","# Bước đầu là tính toán error cho lớp neuron output, từ đó sẽ cho chúng ta biết neuron nào bị sai lệch (error) nhiều để lan truyền (propagation) ngược trong network\n","# Công thức tính error: error = (expected - output) * transfer_derivative(output) (1)\n","# Với expected là gt mong đợi, output là gt thực tế.\n","# Tuy nhiên, với các lớp hidden, công thức sẽ có chút khác biệt\n","# Công thức tính error: error = (weight_k * error_j) * transfer_derivative(output) (2)\n","# Với weight_k là weight mà kết nối từ neuron thứ k của layer trước với neuron hiện tại, error_j là error của neuron thứ j của layer trước và output là output của neuron hiện thời\n"," \n","def backward_propagate_error(network, expected): #1 network sẽ gồm nhiều layer, 1 layer sẽ gồm nhiều neuron, 1 neuron sẽ bao gồm nhiều thuộc tính như output, weight or error(delta)\n","# ta sẽ mặc định network[-1] là layer output, còn lại là hidden, 1 neuron sẽ có output là pt thứ 0, xong tiếp theo là dãy weight (pt thứ 1) và cuối cùng là delta errors (pt thứ 2, tuy nhiên ở layer output sẽ k có thuộc tính này)\n","  for i in reversed(range(len(network))):\n","    layer = network[i]\n","    errors = list()\n","    if i == len(network)-1: #trường hợp ở output layer\n","      for j in range(len(layer)):\n","        neuron = layer[j] #mặc định\n","        #print(neuron[0])\n","        errors.append(expected[j] - neuron[0]) #theo công thức (1)\n","    else: #trường hợp ở hidden layer\n","      for j in range(len(layer)):\n","        error = 0.0\n","        for neuron in network[i+1]:\n","          print(neuron)\n","          error += (neuron[1][j]*neuron[2]) #theo công thức 2\n","        errors.append(error)\n","    for j in range(len(layer)):\n","      neuron = layer[j] #lấy neuron là layer[j] chứ k phải tạo bản sao\n","      #print(errors[j]*transfer_derivative(neuron[0]))\n","      neuron.append(errors[j]*transfer_derivative(neuron[0]))\n"," \n","# network = [[[0.7105668883115941, [0.13436424411240122, 0.8474337369372327, 0.763774618976614]]],\n","#       [[0.6213859615555266, [0.2550690257394217, 0.49543508709194095]], [0.6573693455986976,[0.4494910647887381, 0.651592972722763]]]]\n","# network=[[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n","#       [{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n","# expected = [0, 1]\n"," \n","# backward_propagate_error(network, expected)\n","# for layer in network:\n","#   print(layer)\n"," \n","# ********************************* Train Network ***************************************\n","# Sử dụng SGD, hay còn gọi là tách training set ra thành các batch, mỗi batch có size là 1.\n","# Quy trình: Forward inputs qua các neuron để tạo các weights, sau đó backpropagating error và cập nhật weights\n","# Chúng ta chia làm 2 phần:\n","# 1. Update weights (cập nhật weights): #công thức weight = weight + learning_rate*error*input\n","# trong đó: learning_rate giới hạn độ thay đổi là nhanh hay chậm, error tính từ backpropagation và input tương ứng\n","def update_weights(network, row, l_rate):\n","  eta = l_rate\n","  for i in range(len(network)):\n","    inputs = row[:-1]\n","    if i != 0:\n","      inputs = [neuron[0] for neuron in network[i-1]]\n","    for neuron in network[i]:\n","      for j in range(len(inputs)):\n","        neuron[1][j] += eta*neuron[2]*inputs[j]\n","      neuron[1][-1] += eta*neuron[2]\n"," \n","# 2. Train network: sử dụng SGD, gồm 1 số lượng cố định số lượng các epochs và cập nhật mỗi epoch theo hàng trong dataset\n","# expected number of output (n_outputs) được dùng để biến đổi lớp giá trị đầu vào thành vector binary (one hot coding)\n","def training_network(network, trainset, l_rate, n_epoch, n_outputs):\n","  for epoch in range(n_epoch):\n","    sum_error = 0\n","    for row in trainset:\n","      # print(row)\n","      outputs = forward_propagate(network,row)\n","      # print(outputs)\n","      expected = [0 for i in range(n_outputs)]\n","      expected[row[-1]] = 1\n","      sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","      backward_propagate_error(network, expected)\n","      update_weights(network, row, l_rate)\n","    print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"OuXCtlYDSY0Y","executionInfo":{"status":"error","timestamp":1605803225110,"user_tz":-420,"elapsed":1108,"user":{"displayName":"An Nguyễn Thế","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiONWb6mE3SbdmiD0BUtth9eMBYJK4mRt8PKEgfBQ=s64","userId":"04882142670331807326"}},"outputId":"68d3c3c7-0968-461c-9c8b-ed52e137f8c8"},"source":["seed(1)\n","dataset = [[2.7810836,2.550537003,0],\n","    [1.465489372,2.362125076,0],\n","    [3.396561688,4.400293529,0],\n","    [1.38807019,1.850220317,0],\n","    [3.06407232,3.005305973,0],\n","    [7.627531214,2.759262235,1],\n","    [5.332441248,2.088626775,1],\n","    [6.922596716,1.77106367,1],\n","    [8.675418651,-0.242068655,1],\n","    [7.673756466,3.508563011,1]]\n","n_inputs = len(dataset[0]) - 1 #số lượng feature - số lượng input đầu vào = 2 (-1 là output)\n","n_outputs = len(set([row[-1] for row in dataset])) #do chỉ có giá trị 0 và 1\n","# print(n_inputs,n_outputs)\n","network = initialize_network(n_inputs, 2, n_outputs) #2 input (1 layer), 2 hidden (1 layer), 2 output (1 layer)\n","# => tồn tại 2 neuron với hidden và 1 output với 2 neuron, mỗi neuron sẽ có 3 attribute\n","# vì sao lại có 3 attribute vì có 2 feature + 1 bias\n","# for layer in network:\n","#   print(layer)\n","# print(network)\n","training_network(network, dataset, 0.5, 20, n_outputs)\n","# for layer in network:\n","    # print(layer)\n"," \n","# seed(1)\n","# net = initialize_network(2,1,2)\n","# for layer in net:\n","# print(layer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.07909521373384375]\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-82cebffe4cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#   print(layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# print(network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtraining_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# for layer in network:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# print(layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-75ec62996898>\u001b[0m in \u001b[0;36mtraining_network\u001b[0;34m(network, trainset, l_rate, n_epoch, n_outputs)\u001b[0m\n\u001b[1;32m    138\u001b[0m       \u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0msum_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m       \u001b[0mbackward_propagate_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m       \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>epoch=%d, lrate=%.3f, error=%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-75ec62996898>\u001b[0m in \u001b[0;36mbackward_propagate_error\u001b[0;34m(network, expected)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m           \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#theo công thức 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3ZZk-_xS_9E","executionInfo":{"status":"ok","timestamp":1605803003369,"user_tz":-420,"elapsed":806,"user":{"displayName":"An Nguyễn Thế","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiONWb6mE3SbdmiD0BUtth9eMBYJK4mRt8PKEgfBQ=s64","userId":"04882142670331807326"}},"outputId":"b0b2fc08-7fa0-45a2-946b-b4bf0c8d6fc0"},"source":["import numpy as np\n","# np.array(network).shape\n","print(len(network))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"34Vf6WFIO3Ec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605803109946,"user_tz":-420,"elapsed":852,"user":{"displayName":"An Nguyễn Thế","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiONWb6mE3SbdmiD0BUtth9eMBYJK4mRt8PKEgfBQ=s64","userId":"04882142670331807326"}},"outputId":"1db5d79e-695d-490b-9752-6a5cd741f17f"},"source":["from math import exp\n","import numpy as np\n","from random import seed\n","from random import random\n"," \n","# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","\tnetwork = list()\n","\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","\tnetwork.append(hidden_layer)\n","\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","\tnetwork.append(output_layer)\n","\treturn network\n"," \n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","\tactivation = weights[-1]\n","\tfor i in range(len(weights)-1):\n","\t\tactivation += weights[i] * inputs[i]\n","\treturn activation\n"," \n","# Transfer neuron activation\n","def transfer(activation):\n","\treturn 1.0 / (1.0 + exp(-activation))\n"," \n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","\tinputs = row\n","\tfor layer in network:\n","\t\tnew_inputs = []\n","\t\tfor neuron in layer:\n","\t\t\tactivation = activate(neuron['weights'], inputs)\n","\t\t\tneuron['output'] = transfer(activation)\n","\t\t\tnew_inputs.append(neuron['output'])\n","\t\tinputs = new_inputs\n","\treturn inputs\n"," \n","# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","\treturn output * (1.0 - output)\n"," \n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","\tfor i in reversed(range(len(network))):\n","\t\tlayer = network[i]\n","\t\terrors = list()\n","\t\tif i != len(network)-1:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\terror = 0.0\n","\t\t\t\tfor neuron in network[i + 1]:\n","\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n","\t\t\t\terrors.append(error)\n","\t\telse:\n","\t\t\tfor j in range(len(layer)):\n","\t\t\t\tneuron = layer[j]\n","\t\t\t\terrors.append(expected[j] - neuron['output'])\n","\t\tfor j in range(len(layer)):\n","\t\t\tneuron = layer[j]\n","\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n"," \n","# Update network weights with error\n","def update_weights(network, row, l_rate):\n","\tfor i in range(len(network)):\n","\t\tinputs = row[:-1]\n","\t\tif i != 0:\n","\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n","\t\tfor neuron in network[i]:\n","\t\t\tfor j in range(len(inputs)):\n","\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n","\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n"," \n","# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","\tfor epoch in range(n_epoch):\n","\t\tsum_error = 0\n","\t\tfor row in train:\n","\t\t\toutputs = forward_propagate(network, row)\n","\t\t\texpected = [0 for i in range(n_outputs)]\n","\t\t\texpected[row[-1]] = 1\n","\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","\t\t\tbackward_propagate_error(network, expected)\n","\t\t\tupdate_weights(network, row, l_rate)\n","\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n"," \n","# Test training backprop algorithm\n","seed(1)\n","dataset = [[2.7810836,2.550537003,0],\n","\t[1.465489372,2.362125076,0],\n","\t[3.396561688,4.400293529,0],\n","\t[1.38807019,1.850220317,0],\n","\t[3.06407232,3.005305973,0],\n","\t[7.627531214,2.759262235,1],\n","\t[5.332441248,2.088626775,1],\n","\t[6.922596716,1.77106367,1],\n","\t[8.675418651,-0.242068655,1],\n","\t[7.673756466,3.508563011,1]]\n","n_inputs = len(dataset[0]) - 1\n","n_outputs = len(set([row[-1] for row in dataset]))\n","network = initialize_network(n_inputs, 2, n_outputs)\n","# for layer in network:\n","#   print(\"layer\")\n","#   for neuron in layer:\n","#     print(\"Output:\",neuron['output'], \"Weights:\",neuron['weights'])\n","for layer in network:\n","  print(layer)\n","train_network(network, dataset, 0.5, 20, n_outputs)\n","for layer in network:\n","\tprint(layer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2, 2)\n","[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}, {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]}]\n","[{'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}, {'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}]\n",">epoch=0, lrate=0.500, error=6.350\n",">epoch=1, lrate=0.500, error=5.531\n",">epoch=2, lrate=0.500, error=5.221\n",">epoch=3, lrate=0.500, error=4.951\n",">epoch=4, lrate=0.500, error=4.519\n",">epoch=5, lrate=0.500, error=4.173\n",">epoch=6, lrate=0.500, error=3.835\n",">epoch=7, lrate=0.500, error=3.506\n",">epoch=8, lrate=0.500, error=3.192\n",">epoch=9, lrate=0.500, error=2.898\n",">epoch=10, lrate=0.500, error=2.626\n",">epoch=11, lrate=0.500, error=2.377\n",">epoch=12, lrate=0.500, error=2.153\n",">epoch=13, lrate=0.500, error=1.953\n",">epoch=14, lrate=0.500, error=1.774\n",">epoch=15, lrate=0.500, error=1.614\n",">epoch=16, lrate=0.500, error=1.472\n",">epoch=17, lrate=0.500, error=1.346\n",">epoch=18, lrate=0.500, error=1.233\n",">epoch=19, lrate=0.500, error=1.132\n","[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': -0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': 0.0026279652850863837}]\n","[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': -0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': 0.03803132596437354}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zoag76SY1zaD"},"source":[""]},{"cell_type":"code","metadata":{"id":"UqP3IIT7S8af"},"source":[""],"execution_count":null,"outputs":[]}]}