{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Descent.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNlWpmqjE27HqQuJwGy6nKo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Ox03G8isRPj_"},"source":["#GD for 1-var\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","#hàm số y = x^2 + 5sin(x)\n","def grad(x):\n","  return 2*x+ 5*np.cos(x)\n","def hs(x):\n","  return x**2 + 5*np.sin(x)\n","def GD(eta, x0):\n","  x =[x0]\n","  for i in range (100):\n","    #công thức: x(1) = x(0)-n*f'(x(0)) với n = eta\n","    x_new = x[-1]-eta*grad(x[-1])\n","    if(abs(grad(x_new)) < 0.001):\n","      break\n","    x.append(x_new)\n","  return (x[-1],i)\n","(x1,iter1) = GD(0.1,5)\n","print(\"Ket qua thu duoc x =\",x1,\"sau\",iter1,\"lan chay voi nghiem hs ban dau la\",hs(x1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUeKZmOIRXd_"},"source":["#GD for multivar\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from sklearn import linear_model\n","#hàm số y = 3x+4\n","#Linear Regression\n","np.random.seed(1) #hàm để config sinh ngẫu nhiên (1-32768)\n","X = np.random.rand(1000, 1) #random các số trong khoảng 0-1, với 1000 hàng và 1 cột\n","y = 4 + 3 * X + .2*np.random.randn(1000, 1) # noise added, sinh test\n","regr = linear_model.LinearRegression().fit(X,y)\n","x0 = np.linspace(0, 1, 2, endpoint=True)\n","y0 = regr.intercept_ + regr.coef_[-1]*x0 #-1 để lấy phần tử cuối cùng của coef (Do hàm 1 tham số y = ax+b nên chỉ lấy 1 phần tử)\n","print(regr.intercept_,\" \",regr.coef_)\n","plt.plot(X.T, y.T, 'b.')     # data \n","plt.plot(x0, y0, 'y', linewidth = 2)   # the fitting line\n","plt.axis([0, 1, 3, 9])\n","plt.show()\n","print(\"Solution by GD\")\n","#Gradient Descent\n","#Cost Function: 1/2n * sigma(y-y')^2 với y' = X*weight\n","#=> Gradient: 1/n * X.T * sigma(y'-y)\n","one = np.ones((X.shape[0],1))\n","Xbar = np.concatenate((one, X), axis = 1)\n","reg = linear_model.LinearRegression(fit_intercept=False).fit(X,y)\n","w = reg.coef_\n","one = np.ones((X.shape[0],1))\n","X = np.concatenate((one,X), axis = 1)\n","def grad(w):\n","  N = 1000 #1000 hàng, 1000 samples\n","  return 1/N * X.T.dot(X.dot(w)-y)\n","def cost(w):\n","  return 1/(2*N) * ((x.dot(w)-y)**2)\n","#chọn w_init là [[2],[1]], learning_rate = 1\n","def GD(w_init, grad, eta):\n","  w = [w_init]\n","  for it in range (100): #normal distribution\n","    w_new = w[-1] - eta*grad(w[-1]) #x_t+1 = x_t + eta*gradient(x_t)\n","    if np.linalg.norm(grad(w_new))/len(w_new) < 1e-4: #norm tương tự như abs của grad nhưng là cho ma trận\n","      break #công thức ktra: norm của grad(x_new)/len(x_new)\n","    w.append(w_new)\n","  return (w[-1],it)\n","(res,itr) = GD([[2],[1]],grad,1)\n","print(\"weight:\",res,\"after:\",itr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ie5ECwvpRalg"},"source":["#GD tunning\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from sklearn import linear_model\n","#with momentum: \n","# v_t = v_(t-1)*gamma + eta*gradient(v_t) (gamme thường là 0.9) => tìm được global minimun (Có thể)\n","# => w = w-v_t\n","def GD_momentum(w_init, grad, eta, gamma):\n","    w = [w_init] \n","    v = [np.zeros_like(w_init)] #tạo một ma trận 0 với kích thước như w_init\n","    for it in range(100):\n","        v_new = gamma*v[-1] + eta*grad(w[-1])\n","        w_new = w[-1] - v_new\n","        if np.linalg.norm(grad(w_new))/len(w_new) < 1e-3:\n","            break\n","        w.append(w_new)\n","        v.append(v_new)\n","    return (w[-1], it)\n","w_init = np.array([[2], [1]])\n","#(w_mm, it_mm) = GD_momentum(w_init, grad, 0.5, 0.9)\n","\n","#NAG (Nesterov acclerated gradient) => hội tụ nhanh hơn momentum\n","#v_t = v_(t-1)*gamma + eta*gradient(w-v_(t-1)*gamma)\n","#=> w = w - v_t\n","def GD_NAG(w_init, grad, eta, gamma):\n","    w = [w_init]\n","    v = [np.zeros_like(w_init)]\n","    for it in range(100):\n","        v_new = gamma*v[-1] + eta*grad(w[-1] - gamma*v[-1])\n","        w_new = w[-1] - v_new\n","        if np.linalg.norm(grad(w_new))/len(w_new) < 1e-3:\n","            break\n","        w.append(w_new)\n","        v.append(v_new)\n","    return (w[-1], it)\n","w_init = np.array([[2], [1]])\n","#(w_mm, it_mm) = GD_NAG(w_init, grad, .5, 0.9)\n","\n"],"execution_count":null,"outputs":[]}]}